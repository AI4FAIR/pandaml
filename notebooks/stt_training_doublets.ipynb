{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Training: DNN Tracking on STT (Hit Pairs)_\n",
    "\n",
    "Waleed's TrackML like solution. Its similar to TrackML second solution (difference is in feature columns, prob table, hits in adjacent layers).\n",
    "\n",
    "- Dense Neural Network (DNN)\n",
    "- Hit Pair Probability Table\n",
    "- Cluster Hits into Tracks\n",
    "\n",
    "Inputs (features) and Targets (Labels) for this method are;\n",
    "\n",
    "$$X = (h_{i}, h_{j})$$\n",
    "$$y = 1 / 0$$\n",
    "\n",
    "Where $h_{i}$ = __*(hit_id, x, y, skewed, layer_id, sector_id, isochrone_radius, particle_id)*__ etc. We can add more information in a hit.\n",
    "\n",
    "Note\n",
    "- Train[:,:-1] all columns except last one (-1), Train[:,-1] only last column (-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _For validation accuracy $\\ge$ training accuracy_\n",
    "\n",
    "- [stackoverflow.com](https://stackoverflow.com/questions/43979449/higher-validation-accuracy-than-training-accurracy-using-tensorflow-and-keras)\n",
    "- [keras.io](https://keras.io/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss)\n",
    "- [cs231n.github.io](http://cs231n.github.io/neural-networks-3/#sanitycheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Network Performance_\n",
    "\n",
    "- We would expect many more epochs needed to minimize the error or we may just have too many hidden nodes.\n",
    "- If an epoch consists of $11 \\times 10^6$ examples, then most of the update work might already be done within the $1^{st}$ epoch. In that case, a **loss graph** over **training example number** rather than over epochs might be more instructive to see initial training behaviour.\n",
    "- Did you use [early stopping](https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/) to avoid overtraining? **Issue:** it might not use whole data set. **Solution:** use epoch number as hyeprparameter.\n",
    "- Hyperparameter Tuning (**GridSearchCV**)\n",
    "- Maybe some data preprocessing might help the network to ease the task of implicit data preprocessing. In other words, you might want to apply a standardizing [(log-)z-score transformation](https://en.wikipedia.org/wiki/Standard_score) before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from tqdm import tnrange\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_models import init_model, init_model_dnn\n",
    "from prepare import dask_events, load_events, preprocess, prepare_train_set, prepare_test_set\n",
    "from drawing import draw_train_history, draw_pickled_history, draw_single_event, draw_reco_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras as k\n",
    "# import tensorflow as tf\n",
    "# print(\"tensorflow:\", tf.__version__, \", tf.keras:\", tf.keras.__version__, \", keras:\", k.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Params/Configs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "epochs=10\n",
    "val_frac=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.environ['HOME']+'/current/data_sets/'+'mljuelich/data2/'\n",
    "path = os.environ['HOME']+'/current/2_deepana/pandaml/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.environ['HOME']+'/current/2_deepana/pandaml/models'):\n",
    "    os.mkdir('models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Load Events_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load single event using event_prefix\n",
    "def get_event(path, event_prefix):\n",
    "    hits= pd.read_csv(path+'%s-hits.csv'%event_prefix)\n",
    "    tubes= pd.read_csv(path+'%s-tubes.csv'%event_prefix)\n",
    "    truth= pd.read_csv(path+'%s-truth.csv'%event_prefix)\n",
    "    particles= pd.read_csv(path+'%s-particles.csv'%event_prefix)\n",
    "    \n",
    "    return hits, tubes, truth, particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Event Vizualization_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id = 1\n",
    "event_prefix = 'event00000000%02d'%event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, tubes, truth, particles = get_event(path, event_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits.particle_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_event(hits, event_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Preprocessing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hits.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hits.hit_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tubes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hits = hits.drop(['volume_id','depcharge','particle_id'], axis=1)\n",
    "\n",
    "# df = event[['skewed', 'layer_id']]   # select only 2 columns\n",
    "# df = df[df['skewed'] == 0]           # all non skewed\n",
    "# df = df.query('skewed==0')\n",
    "# df = df[df['skewed'] == 1]           # all skewed, layers = 8,9,2,...15 are skewed\n",
    "# df = df.query('skewed==1')\n",
    "\n",
    "# df = df[df['layer_id'] < 8]          # layers = 0,1,2,...7 are before skewed\n",
    "# df = df[df['layer_id'] > 15]         # layers = 16,17,...24 are after skewed\n",
    "# df = df[(df['layer_id'] > 7) & \n",
    "# (df['layer_id'] < 16)]               # layers = 8,9,2,...15 are skewed\n",
    "\n",
    "# df.cluster_id.unique()\n",
    "# df.layer.unique()\n",
    "# df[df[\"layer\"] < 3].head()\n",
    "# df[(df[\"layer\"] < 5) & (df[\"layer\"] > 3)].head()\n",
    "# df.loc[df[\"layer\"].isin([2, 3])].head()\n",
    "# df.loc[~df[\"layer\"].isin([2, 3])].head()\n",
    "# df.loc[(df[\"cluster_id\"]==8) & df[\"layer\"].isin([2, 3])].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Prepare Training Data_\n",
    "\n",
    "Dataset for training (Hit Doublets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can jump to step4 for test only.\n",
    "if train:\n",
    "    Train = []\n",
    "    for i in tqdm_notebook(range(0, 10)):\n",
    "        event_prefix = 'event00000000%02d'%i\n",
    "        hits, tubes, truth, particles = get_event(path, event_prefix)\n",
    "        \n",
    "        # preprocessing tasks\n",
    "        # hits = hits.drop(['volume_id','depcharge','particle_id'], axis=1)\n",
    "        \n",
    "        # create features=[x,y,z, ]\n",
    "        features = np.hstack((hits[['x','y','z']],\n",
    "                              # hits[['tube_id']]/1000,\n",
    "                              # hits[['skewed','layer_id','sector_id']]/10,\n",
    "                              hits[['isochrone', 'energyloss']]))\n",
    "        \n",
    "        particle_ids = truth.particle_id.unique()\n",
    "        particle_ids = particle_ids[np.where(particle_ids!=0)[0]]\n",
    "        pair = []\n",
    "        for particle_id in particle_ids:\n",
    "            hit_ids = truth[truth.particle_id == particle_id].hit_id.values-1\n",
    "            for i in hit_ids:\n",
    "                for j in hit_ids:\n",
    "                    if i != j:\n",
    "                        pair.append([i,j])\n",
    "                        \n",
    "        pair = np.array(pair)\n",
    "        \n",
    "        Train1 = np.hstack((features[pair[:,0]], features[pair[:,1]], np.ones((len(pair),1))))\n",
    "        \n",
    "        if len(Train) == 0:\n",
    "            Train = Train1\n",
    "        else:\n",
    "            Train = np.vstack((Train,Train1))\n",
    "\n",
    "        n = len(hits)\n",
    "        size = len(Train1)*3\n",
    "        p_id = truth.particle_id.values\n",
    "        \n",
    "        # print(\"n: \", n, \"size: \", size, \"p_id: \", p_id)\n",
    "        # randomly choose pair indexes\n",
    "        i =np.random.randint(n, size=size)\n",
    "        j =np.random.randint(n, size=size)\n",
    "        pair = np.hstack((i.reshape(size,1),j.reshape(size,1)))\n",
    "        pair = pair[((p_id[i]==0) | (p_id[i]!=p_id[j]))]\n",
    "\n",
    "        Train0 = np.hstack((features[pair[:,0]], features[pair[:,1]], np.zeros((len(pair),1))))\n",
    "\n",
    "        print(event_prefix, Train1.shape)\n",
    "\n",
    "        Train = np.vstack((Train,Train0))\n",
    "\n",
    "    del Train0, Train1\n",
    "\n",
    "    np.random.shuffle(Train)\n",
    "    print(Train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can jump to step4 for test only.\n",
    "if train:\n",
    "    X = []                               # input features as list\n",
    "    y = []                               # output as a list\n",
    "    for i in tqdm_notebook(range(0,10)):\n",
    "        event_prefix = 'event00000000%02d'%i\n",
    "        print(event_prefix)\n",
    "        hits, tubes, truth, particles = get_event(path, event_prefix)\n",
    "        \n",
    "        # feature selection (if more/less are needed)\n",
    "        hits = hits[['hit_id','x','y','z','isochrone','energyloss', 'particle_id']]\n",
    "        data = np.array(hits)\n",
    "        \n",
    "        # PLEAZE check the right indexing of the dataframe (print (df.head(0)))\n",
    "        # 0=hit_id, 1=x, 2=y, 3=skewed, 4=layer_id, 5=sector_id, 6=isochrone radius, 7=particle_id\n",
    "        for i in data:\n",
    "            for j in data:\n",
    "                if (i[0]==j[0]): continue                   # if hits are same then skip\n",
    "                # if (abs(i[4]-j[4])!=1): continue          # only adjacent layers\n",
    "\n",
    "                # distance between hits\n",
    "                dirV_x = j[1]-i[1]\n",
    "                dirV_y = j[2]-i[2]\n",
    "                dist   = math.sqrt(dirV_x**2 + dirV_y**2)\n",
    "                if i[6]==j[6]:\n",
    "                    X.append([i[1], i[2], i[3], i[4], i[5], \n",
    "                              j[1], j[2], j[3], j[4], j[5]]) \n",
    "                              #abs(dist-(i[4]+j[4]))])\n",
    "                    y.append([1])\n",
    "                else:\n",
    "                    X.append([i[1], i[2], i[3], i[4], i[5], \n",
    "                              j[1], j[2], j[3], j[4], j[5]]) \n",
    "                              #abs(dist-(i[4]+j[4]))])\n",
    "                    y.append([0])\n",
    "                    \n",
    "    # Covert X,y list as ndarray\n",
    "    X = np.vstack(X)\n",
    "    y = np.vstack(y)\n",
    "    combined = list(zip(X, y))  # make list of iterators on X,y as tuple, use tuple(combines) to print the values\n",
    "    random.shuffle(combined)    # Shuffle list\n",
    "    X[:], y[:] = zip(*combined) # operator * can be used to unzip a list >> ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Train Model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    # Build Model:\n",
    "    model = init_model_dnn(shape=10)\n",
    "        \n",
    "    # Model Summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Save Model as PNG\n",
    "    plot_model(model, to_file='models/dnn.png', show_shapes=True, show_layer_names=True)\n",
    "    # SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    # Fit the Model\n",
    "    hist = model.fit(x=Train[:,:-1], y=Train[:,-1], epochs=20, batch_size=batch_size, validation_split=val_frac, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    # Plot Train History\n",
    "    draw_train_history(hist);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNVisualizer\n",
    "# from ann_visualizer.visualize import ann_viz\n",
    "# ann_viz(model, view=True,  filename=\"dnn\", title=\"Deep Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if Train:\n",
    "    # Fit the Model\n",
    "    hist = model.fit_generator(generator=train_batcher, steps_per_epoch=epoch_size, epochs=num_epochs,\n",
    "                               verbose=1, validation_data=valid_batcher, validation_steps=val_step, \n",
    "                               use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    # Fit the Model\n",
    "    hist = model.fit(X, y, epochs=20, batch_size=batch_size, validation_split=val_frac, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    # Plot Train History\n",
    "    draw_train_history(hist);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    # Save the Model\n",
    "    model.save('models/mymodel.h5')\n",
    "    \n",
    "    # Save History (Using Pickle)\n",
    "    with open('models/mymodel.pickle', 'wb') as hist_file:\n",
    "        pickle.dump(hist.history, hist_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Load Model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Load=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Load:\n",
    "    history = None\n",
    "    with open('models/dnn_inout.pickle', 'rb') as pickle_in:\n",
    "        history = pickle.load(pickle_in)\n",
    "    draw_pickled_history(history, draw_val=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Load:\n",
    "    # model = load_model('models/init_model_dnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Test Model_\n",
    "\n",
    "*Accuracy/Classification:* `np.mean(y_pred == y_test)`\n",
    "*Missclassification:* `np.mean(y_pred != y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Model Metrics\n",
    "# model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set loss and accuracy\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test_set loss and accuracy\n",
    "# print(\"test_loss:\", test_loss, \", test_acc:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Model Predictions_\n",
    "\n",
    "*Prediction:* `y_pred = model.predict(X_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test set predictions\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _SttHitClusterer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SttHitClusterer:\n",
    "    def __init__(self, path, model=None, batch_size=128, num_epochs=10):\n",
    "        \n",
    "        # Build Model:\n",
    "        self.model = model\n",
    "        # self.model = build_model(self.hidden_dim, DetectorGeometry.layer_r.shape[0], self.num_phi_bin)\n",
    "        # self.model = init_model_dnn(shape=10)\n",
    "        \n",
    "        # Params:\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.val_frac = 0.2\n",
    "\n",
    "        # Params:\n",
    "        self.train_input = None\n",
    "        self.train_target = None\n",
    "        self.test_input = None\n",
    "        self.test_pred = None\n",
    "        \n",
    "        # Params:\n",
    "        self.history = None\n",
    "        self.prepared = False\n",
    "    \n",
    "    def prepare_data(self, events, n_events):\n",
    "        if self.prepared:\n",
    "            return\n",
    "        self.prepared = True\n",
    "        self.train_input, train_target = prepare_pairs(events=events, n_events=n_events)\n",
    "        \n",
    "    def fit(self, events, n_events):\n",
    "        \n",
    "        # Prepare Data\n",
    "        prepare_data(self, events, n_events)\n",
    "        \n",
    "        # Fit DNN Model\n",
    "        self.history = self.model.fit(x=self.train_input,\n",
    "                                      y=self.train_target,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      epochs=self.num_epochs,\n",
    "                                      validation_split=self.val_frac)\n",
    "        # Save the Model\n",
    "        self.model.save(\"model.h5\")\n",
    "        \n",
    "    def predict_single_event(self, x_event):\n",
    "        print(\"TODO: predict single event\")\n",
    "        \n",
    "    def param_summary(self):\n",
    "        print(\"batch_size:\", self.batch_size)\n",
    "        print(\"num_epochs:\", self.num_epochs)\n",
    "        print(\"val. fraction:\", self.val_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust = SttHitClusterer(path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust.param_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
